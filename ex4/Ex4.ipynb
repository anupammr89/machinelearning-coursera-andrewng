{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Neural Networks Learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the backpropagation algorithm for neural networks and apply it to the task of hand-written digit recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import scipy.optimize as op\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from a matlab file. The data is in the dictionary format with X representing 5000 20x20 pixel images in a 5000 x 400 matrix and y representing the number corresponding to the image in 5000 x 1 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'X', 'y'])\n",
      "(5000, 400) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "pixelsmat = scipy.io.loadmat('ex4data1.mat')\n",
    "print(pixelsmat.keys())\n",
    "X = pixelsmat['X']\n",
    "y = pixelsmat['y']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the different classes in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different classes {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n"
     ]
    }
   ],
   "source": [
    "print(\"Different classes\", set(y.reshape(y.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the number of samples and pixels variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 20\n"
     ]
    }
   ],
   "source": [
    "numsamples = X.shape[0]\n",
    "numpixels1d = X.shape[1]\n",
    "numpixels2d = int(np.sqrt(numpixels1d))\n",
    "print(numpixels1d, numpixels2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize images using randomly selected 100 rows from X matrix. The image can be displayed using imshow function available in matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img = 100\n",
    "rows = cols = int(np.sqrt(num_img))\n",
    "indices = np.random.randint(0,numsamples, size=num_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgArr = np.zeros((num_img, numpixels1d))\n",
    "for i in range(np.size(indices)):\n",
    "    imgArr[i] = X[indices[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d29cbd6fa134d4db5633aeb53521c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "for i in range(1, np.size(indices)+1):\n",
    "    img = imgArr[i-1].reshape(numpixels2d, numpixels2d)\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.T, cmap='gray', origin=\"upper\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the images are of size 20 x 20, this gives us 400 input layer units. The ex4weights.mat file contains already trained paramenters $\\theta^{(1)}$ and $\\theta^{(2)}$ which are of dimensions 25 x 401 and 10 x 26 accounting the extra bias unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NeuralNetworkModel.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The units in all layers except input is calculated as sigmoid function of weighted sum of inputs\n",
    "<br>The hypothesis is defined as\n",
    "<br>$$ a = g(\\theta^Tx) $$\n",
    "<br>where g is the Sigmoid function\n",
    "<br>$$ g(z) = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the weights into Theta1 and Theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'Theta1', 'Theta2'])\n",
      "(25, 401) (10, 26)\n"
     ]
    }
   ],
   "source": [
    "weightsmat = scipy.io.loadmat('ex4weights.mat')\n",
    "print(weightsmat.keys())\n",
    "Theta1 = weightsmat['Theta1']\n",
    "Theta2 = weightsmat['Theta2']\n",
    "print(Theta1.shape, Theta2.shape)\n",
    "tc = 0\n",
    "tg = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append bias unit to X matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 shape (5000, 401)\n"
     ]
    }
   ],
   "source": [
    "y_set = set(y.reshape(y.shape[0]))\n",
    "X1 = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "print(\"X1 shape\", X1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the given data:\n",
    "* A sample in matrix X which corresponds to first layer is of size (401, 1).\n",
    "* The weights for first layer is of size (25, 401).\n",
    "* The second layer is of size (25, 1).\n",
    "[//]: #\n",
    "Thus for __each sample__, the second layer can be calculated using vectorized implementation as $ a^{(2)} = \\theta^{(1)}.a^{(1)} $\n",
    "<br> In general, for each sample, a layer _l_ can be calculated as $ a^{(l)} = \\theta^{(l-1)}.a^{(l-1)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1/(1+(np.exp(-z))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwdpropgate(theta, x):\n",
    "    z = x.dot(theta.T)\n",
    "    a = sigmoid(z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>The cost function is \n",
    "<br>$$ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K[-y_k^{(i)}log((h_\\theta(x^{(i)}))_k) - (1 - y_k^{(i)})log(1 - (h_\\theta(x^{(i)}))_k] + R$$\n",
    "$$ R = \\frac{\\lambda}{2m}[\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(\\theta_{ji}^{(l)})^2] $$\n",
    "<br> $m$ - number of samples, $K$ - number of classes, $L$ - number of Layers, $s_l$ - number of units in Layer $l$, $\\lambda$ - Regularization factor\n",
    "<br> $R$ is the regularization term which excludes the bias values\n",
    "<br> In our case with one input layer and one hidden layer, this can be rewritten as\n",
    "$$ R = \\frac{\\lambda}{2m}[\\sum_{j=1}^{25}\\sum_{k=1}^{400}(\\theta_{jk}^{(1)})^2 + \\sum_{j=1}^{10}\\sum_{k=1}^{25}(\\theta_{jk}^{(2)})^2] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getThetaFromThetaVec(ThetaVec, Theta1Size, Theta2Size):\n",
    "    # Compute length to split Theta parameters from unrolled vector\n",
    "    Theta1Len = Theta1Size[0]*Theta1Size[1]\n",
    "    # Get Theta1 and Theta2\n",
    "    Theta1 = ThetaVec[:Theta1Len].reshape(Theta1Size)\n",
    "    Theta2 = ThetaVec[Theta1Len:].reshape(Theta2Size)\n",
    "    return (Theta1, Theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumSamplesFeaturesClasses(X, y):\n",
    "    num_samples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    num_classes = len(set(y.reshape(y.shape[0])))\n",
    "    return (num_samples, num_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEncodedy(y, num_classes):\n",
    "    y_mod = y.ravel() - 1\n",
    "    yv = np.eye(num_classes)[y_mod]\n",
    "    return yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ypred(Theta1, Theta2, x):\n",
    "    # Compute predicted values using forward propogation\n",
    "    # This will output predicted values for all samples\n",
    "    # using matrix multiplication\n",
    "    a2 = fwdpropgate(Theta1, x)\n",
    "    a2 = np.hstack((np.ones((x.shape[0], 1)), a2))\n",
    "    a3 = fwdpropgate(Theta2, a2)\n",
    "    return (a2,a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computecost(yv, yhat):\n",
    "    '''\n",
    "    \n",
    "        yhat represents output unit matrix. Its dimension is (m x K)\n",
    "        yv represents one hot encoded matrix based on actual y values. Its dimension is (m x K)\n",
    "\n",
    "        The double sum can be computed as element-wise scalar product for each element in the matrices\n",
    "        followed by sum of all elements in the resultant matrix\n",
    "        Example m = 3, K = 2. Both A and B are 2D arrays of size (m x K)\n",
    "        A = [             B = [\n",
    "                a b              m n\n",
    "                c d              o p\n",
    "                e f              q r\n",
    "            ]                 ]\n",
    "\n",
    "        DoubleSum = a.m + b.n + c.o + d.p + e.q + f.r\n",
    "        For efficient implementation, this is done through, dot product of flattened vectors of length m.K\n",
    "        \n",
    "        For more details, please check https://www.coursera.org/learn/machine-learning/discussions/all/threads/AzIrrO7wEeaV3gonaJwAFA\n",
    "        \n",
    "    '''\n",
    "    cost = -(yv.ravel().dot(np.log(yhat.ravel())) + (1 - yv.ravel()).dot(np.log(1 -yhat.ravel())))\n",
    "    return (cost/yv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeregterm(Theta1, Theta2, l, num_samples):\n",
    "    # remove the bias column for both theta vectors and flatten it\n",
    "    Theta1 = Theta1[:,1:].flatten()\n",
    "    Theta2 = Theta2[:,1:].flatten()\n",
    "    regparam = Theta1.T.dot(Theta1) + Theta2.T.dot(Theta2)\n",
    "    return ((l*regparam)/(2*num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(ThetaVec, Theta1Size, Theta2Size, X, y, l, pa):\n",
    "    global tc\n",
    "    st = time.time()\n",
    "    # Get Theta1 (25 x 401) and Theta2 (10 x 26)\n",
    "    (Theta1, Theta2) = getThetaFromThetaVec(ThetaVec, Theta1Size, Theta2Size)\n",
    "    # Get number of samples, features and classes\n",
    "    (num_samples, num_features, num_classes) = getNumSamplesFeaturesClasses(X, y)\n",
    "    \n",
    "    # Compute hidden and output layer matrix\n",
    "    # X  - 5000 x 401, Theta1 - 25 x 401, a2 - 5000 x 26 after adding bias column\n",
    "    # a2 - 5000 x 26,  Theta2 - 10 x 26,  a3(yhat) - 5000 x 10\n",
    "    a2, yhat = ypred(Theta1, Theta2, X)\n",
    "    \n",
    "    # Create a \"One hot encoded\" matrix based in classes in y vector\n",
    "    yv = getEncodedy(y, num_classes)\n",
    "    # Compute cost\n",
    "    cost = (computecost(yv, yhat))\n",
    "    # Compute accuracy\n",
    "    if(pa == 1):\n",
    "        max_ind = np.argmax(yhat, axis = 1)\n",
    "        count = np.sum(y.ravel() == (max_ind+1))\n",
    "    # Account regularization into cost\n",
    "    if(l > 0):\n",
    "        cost += computeregterm(Theta1, Theta2, l, num_samples)\n",
    "    # Compute accuracy\n",
    "    if(pa == 1):\n",
    "        accuracy = (count/num_samples)\n",
    "        print(\"Accuracy is {:%}, Cost is {}\".format(accuracy, cost))\n",
    "    et = time.time()\n",
    "    tc += (et - st)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost without regularization is 0.28762916516131887\n",
      "Accuracy is 97.520000%, Cost is 0.38376985909092365\n",
      "Cost with regularization 0.38376985909092365\n"
     ]
    }
   ],
   "source": [
    "# Unroll Theta1 and Theta2 into single vector\n",
    "ThetaVec = np.concatenate((Theta1.ravel(), Theta2.ravel()))\n",
    "# Cost without regularization\n",
    "print(\"Cost without regularization is\", cost(ThetaVec, Theta1.shape, Theta2.shape, X1, y, 0, 0))\n",
    "# Cost with regularization\n",
    "print(\"Cost with regularization\", cost(ThetaVec, Theta1.shape, Theta2.shape, X1, y, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation allows to calculate gradient of the cost function which in turn will be used along with the above cost function by the optimization function to calculate the optimum Theta vectors.\n",
    "1. Implement backpropagation to compute partial derivatives.\n",
    "    * Implement sigmoid gradient function $g'(z) = \\frac{d}{dz}g(z) = g(z)(1 - g(z))$\n",
    "    * Randomly initialize the theta vectors for symmetry breaking.\n",
    "2. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.\n",
    "3. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidgrad(a):\n",
    "    return (a*(1-a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One efective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network. A good choice can be set as\n",
    "$$\\epsilon_{init} = \\frac{\\sqrt(6)}{\\sqrt(s_l + s_{l+1})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401) (10, 26) (10285,)\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize the weights Theta1 (25 x 401) and Theta2 (10 x 26) to small values\n",
    "# For more info check https://web.stanford.edu/class/ee373b/nninitialization.pdf\n",
    "epsilon_init = 0.12;\n",
    "num_features = X1.shape[1]\n",
    "num_classes = len(set(y.reshape(y.shape[0])))\n",
    "num_units_l1 = num_features\n",
    "num_units_l2 = 25\n",
    "num_units_l3 = num_classes\n",
    "Theta1_r = np.random.random((num_units_l2, num_units_l1)) * 2 * epsilon_init - epsilon_init;\n",
    "Theta2_r = np.random.random((num_units_l3, num_units_l2+1)) * 2 * epsilon_init - epsilon_init;\n",
    "Theta_r = np.concatenate((Theta1_r.ravel(), Theta2_r.ravel()))\n",
    "print(Theta1_r.shape, Theta2_r.shape, Theta_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute gradient, first perform forward propagation and then compute backwards \"error term\" $\\delta_j^{(l)}$ for all units except bias unit from output unit till input unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"BackpropagationUpdates.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient calculation function\n",
    "$$ \\frac{\\partial}{\\partial\\Theta_{ij}^{(l)}}J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m}\\Delta_{ij}^{(l)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ for j = 0$$\n",
    "$$ \\frac{\\partial}{\\partial\\Theta_{ij}^{(l)}}J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m}\\Delta_{ij}^{(l)} + \\frac{\\lambda}{m}\\Theta_{ij}^{(l)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ for j \\ge 1 $$\n",
    "$$ \\Delta^{(l)} = \\Delta^{(l)} + \\delta^{l+1}(a^{(l)})^T $$\n",
    "In our case, for efficient vectorized implementation,\n",
    "$$ \\delta^{(3)} = a^{(3)} - y $$\n",
    "$$ \\delta^{(2)} = \\delta^{(3)}.\\Theta^2 * g'(z^{(2)}) $$\n",
    "$$ \\Delta^{(1)} = (\\delta^{(2)})^T.X $$\n",
    "$$ \\Delta^{(2)} = (\\delta^{(3)})^T.a^{(2)} $$\n",
    "<br> $m$ - number of samples, $\\lambda$ - Regularization factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(ThetaVec, Theta1Size, Theta2Size, X, y, l, pa):\n",
    "    global tg\n",
    "    st = time.time()\n",
    "    # Get Theta1 (25 x 401) and Theta2 (10 x 26)\n",
    "    (Theta1, Theta2) = getThetaFromThetaVec(ThetaVec, Theta1Size, Theta2Size)\n",
    "    # Get number of samples, features and classes\n",
    "    (num_samples, num_features, num_classes) = getNumSamplesFeaturesClasses(X, y)\n",
    "    # Initialize gradient vectors\n",
    "    grad_d1 = np.zeros((Theta1Size))\n",
    "    grad_d2 = np.zeros((Theta2Size))\n",
    "    # Create a \"One hot encoded\" matrix based in classes in y vector\n",
    "    yv = getEncodedy(y, num_classes)\n",
    "\n",
    "    # 1. Forward propogate to compute hidden (a2 (5000 x 26)) and outer layer (a3/yhat (5000 x 10)) units\n",
    "    a2, yhat = ypred(Theta1, Theta2, X)\n",
    "    \n",
    "    # 2. Backward propogate to compute d3 (5000 x 10) and d2 (5000 x 26) and remove bias column from d2 (5000 x 25)\n",
    "    d3 = yhat - yv\n",
    "    d2 = d3.dot(Theta2) * sigmoidgrad(a2)\n",
    "    d2 = d2[:,1:]\n",
    "\n",
    "    # 3. Compute gradient matrix\n",
    "    grad1 = (d2.T.dot(X))/num_samples\n",
    "    grad2 = (d3.T.dot(a2))/num_samples\n",
    "    \n",
    "    # 4. Account regularization into gradient matrix exluding the bias column\n",
    "    if(l > 0):\n",
    "        grad1[:,1:] += (Theta1[:,1:]*l/num_samples)\n",
    "        grad2[:,1:] += (Theta2[:,1:]*l/num_samples)\n",
    "    # Unroll gradient matrix into a single vector\n",
    "    grad = np.concatenate((grad1.ravel(), grad2.ravel()))\n",
    "    et = time.time()\n",
    "    tg += (et - st)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Checking\n",
    "Sanity method to check if gradient calculation is working correctly\n",
    "<br>Implement function to calculate approximate gradient matrix\n",
    "<br>If backpropagation implementation is correct, the relative diference between gradient and gradientApprox is less than 1e-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientcheck(ThetaVec, Theta1Size, Theta2Size, X, y, l):\n",
    "    # Compute gradient for original theta parameters\n",
    "    grad = gradient(ThetaVec, Theta1Size, Theta2Size, X, y, l, 0)\n",
    "    # Intitialize gradient vector and epsilon as 10^-4\n",
    "    gradApprox = np.zeros((ThetaVec.shape))\n",
    "    epsilon = 0.0001\n",
    "    # Compute cost for gradApprox, by changing one value at a time\n",
    "    for i in range(ThetaVec.shape[0]):\n",
    "        ThetaVecP = np.copy(ThetaVec)\n",
    "        ThetaVecM = np.copy(ThetaVec)\n",
    "        ThetaVecP[i] += epsilon\n",
    "        ThetaVecM[i] -= epsilon\n",
    "        JP = cost(ThetaVecP, Theta1Size, Theta2Size, X, y, l, 0)\n",
    "        JM = cost(ThetaVecM, Theta1Size, Theta2Size, X, y, l, 0)\n",
    "        gradApprox[i] = (JP - JM)/(2*epsilon)\n",
    "    return (grad, gradApprox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random data of small size for gradient checking otherwise it will be very slow and computationally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandData(rows, cols):\n",
    "    num_elems = rows * (cols+1)\n",
    "    W = (np.sin(np.array(np.arange(1,num_elems+1))))/10\n",
    "    return W.reshape(rows, cols+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.39475668e-12 -7.73273806e-13  4.74464218e-13 -1.46139698e-12\n",
      "  4.39316118e-12  9.79648654e-13  4.91589408e-13  1.85249385e-12\n",
      " -6.55514532e-12 -4.03347494e-13  1.21500032e-14 -1.10437874e-12\n",
      " -9.39691033e-12  5.44096018e-13  2.02796287e-12 -4.53751620e-13\n",
      " -5.75027005e-12 -3.02398384e-12  1.24660915e-12  1.69384298e-12\n",
      "  7.93554111e-12  5.33378897e-13  3.12216919e-12 -2.19763097e-12\n",
      "  1.76131332e-12  2.07583950e-13  6.26139418e-12  1.95224392e-12\n",
      " -6.84077794e-13  6.37252751e-12  1.69134151e-12  1.52720891e-12\n",
      "  6.28826158e-12  2.27814989e-12  1.79012360e-12  2.98160108e-12\n",
      "  5.19230492e-13 -1.77566989e-12]\n"
     ]
    }
   ],
   "source": [
    "input_layer_size = 3;\n",
    "hidden_layer_size = 5;\n",
    "num_labels = 3;\n",
    "m = 5;\n",
    "ThetaT1 = getRandData(hidden_layer_size, input_layer_size)\n",
    "ThetaT2 = getRandData(num_labels, hidden_layer_size);\n",
    "XT  = getRandData(m, input_layer_size - 1);\n",
    "XT = np.hstack((np.ones((XT.shape[0], 1)), XT))\n",
    "yt  = (1 + np.mod(np.arange(1,m+1), num_labels)).reshape(m, 1)\n",
    "ThetaVecT = np.concatenate((ThetaT1.ravel(), ThetaT2.ravel()))\n",
    "(grad, gradApprox) = gradientcheck(ThetaVecT, ThetaT1.shape, ThetaT2.shape, XT, yt, 1)\n",
    "print(grad - gradApprox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning parameters using Advanced Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optimization method in scipy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to optimize 2.5985090732574463\n",
      "Total Time take to calculate cost 1.0683214664459229\n",
      "Total Time take to calculate gradient 1.3935165405273438\n",
      "Accuracy is 96.440000%, Cost is 0.4430771510607966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4430771510607966"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 1\n",
    "tc = tg = 0\n",
    "st = time.time()\n",
    "Result = op.minimize(fun = cost, \n",
    "                         x0 = Theta_r, \n",
    "                         args = (Theta1_r.shape, Theta2_r.shape, X1, y, l, 0),\n",
    "                         method = 'CG',\n",
    "                         jac = gradient,\n",
    "                        options = {'maxiter': 50});\n",
    "et = time.time()\n",
    "print(\"Time take to optimize\", (et-st))\n",
    "print(\"Total Time take to calculate cost\", tc)\n",
    "print(\"Total Time take to calculate gradient\", tg)\n",
    "optimal_theta = Result.x;\n",
    "cost(optimal_theta, Theta1_r.shape, Theta2_r.shape, X1, y, l, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

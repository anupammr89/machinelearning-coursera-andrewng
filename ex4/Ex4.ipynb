{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Neural Networks Learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the backpropagation algorithm for neural networks and apply it to the task of hand-written digit recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import scipy.optimize as op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from a matlab file. The data is in the dictionary format with X representing 5000 20x20 pixel images in a 5000 x 400 matrix and y representing the number corresponding to the image in 5000 x 1 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'X', 'y'])\n",
      "(5000, 400) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "pixelsmat = scipy.io.loadmat('ex4data1.mat')\n",
    "print(pixelsmat.keys())\n",
    "X = pixelsmat['X']\n",
    "y = pixelsmat['y']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the different classes in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different classes {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n"
     ]
    }
   ],
   "source": [
    "print(\"Different classes\", set(y.reshape(y.shape[0])))\n",
    "#num_classes = len(set(y.reshape(y.shape[0])))\n",
    "#print(\"Total number of classes\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convinience, lets replace class '10' by class '0', as indexing in python starts from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = np.where(y == 10, 0, y)\n",
    "#print(\"Different classes\", set(y.reshape(y.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the number of samples and pixels variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 20\n"
     ]
    }
   ],
   "source": [
    "numsamples = X.shape[0]\n",
    "numpixels1d = X.shape[1]\n",
    "numpixels2d = int(np.sqrt(numpixels1d))\n",
    "print(numpixels1d, numpixels2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize images using randomly selected 100 rows from X matrix. The image can be displayed using imshow function available in matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img = 100\n",
    "rows = cols = int(np.sqrt(num_img))\n",
    "indices = np.random.randint(0,numsamples, size=num_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgArr = np.zeros((num_img, numpixels1d))\n",
    "for i in range(np.size(indices)):\n",
    "    imgArr[i] = X[indices[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6379144246304efdb805e7918bff336b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "for i in range(1, np.size(indices)+1):\n",
    "    img = imgArr[i-1].reshape(numpixels2d, numpixels2d)\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.T, cmap='gray', origin=\"upper\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the images are of size 20 x 20, this gives us 400 input layer units. The ex4weights.mat file contains already trained paramenters $\\theta^{(1)}$ and $\\theta^{(2)}$ which are of dimensions 25 x 401 and 10 x 26 accounting the extra bias unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NeuralNetworkModel.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The units in all layers except input is calculated as sigmoid function of weighted sum of inputs\n",
    "<br>The hypothesis is defined as\n",
    "<br>$$ a = g(\\theta^Tx) $$\n",
    "<br>where g is the Sigmoid function\n",
    "<br>$$ g(z) = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the weights into Theta1 and Theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'Theta1', 'Theta2'])\n",
      "(25, 401) (10, 26)\n"
     ]
    }
   ],
   "source": [
    "weightsmat = scipy.io.loadmat('ex4weights.mat')\n",
    "print(weightsmat.keys())\n",
    "Theta1 = weightsmat['Theta1']\n",
    "Theta2 = weightsmat['Theta2']\n",
    "print(Theta1.shape, Theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append bias unit to X matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 shape (5000, 401)\n"
     ]
    }
   ],
   "source": [
    "y_set = set(y.reshape(y.shape[0]))\n",
    "X1 = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "print(\"X1 shape\", X1.shape)\n",
    "#num_samples = X1.shape[0]\n",
    "#num_features = X1.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the given data:\n",
    "* A sample in matrix X which corresponds to first layer is of size (401, 1).\n",
    "* The weights for first layer is of size (25, 401).\n",
    "* The second layer is of size (25, 1).\n",
    "[//]: #\n",
    "Thus for __each sample__, the second layer can be calculated using vectorized implementation as $ a^{(2)} = \\theta^{(1)}.a^{(1)} $\n",
    "<br> In general, for each sample, a layer _l_ can be calculated as $ a^{(l)} = \\theta^{(l-1)}.a^{(l-1)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1/(1+(np.exp(-z))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwdpropgate(theta, x):\n",
    "    z = theta.dot(x)\n",
    "    a = sigmoid(z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>The cost function is \n",
    "<br>$$ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K[-y_k^{(i)}log((h_\\theta(x^{(i)}))_k) - (1 - y_k^{(i)})log(1 - (h_\\theta(x^{(i)}))_k] + R$$\n",
    "$$ R = \\frac{\\lambda}{2m}[\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(\\theta_{ji}^{(l)})^2] $$\n",
    "<br> $m$ - number of samples, $K$ - number of classes, $L$ - number of Layers, $s_l$ - number of units in Layer $l$, $\\lambda$ - Regularization factor\n",
    "<br> $R$ is the regularization term which excludes the bias values\n",
    "<br> In our case with one input layer and one hidden layer, this can be rewritten as\n",
    "$$ R = \\frac{\\lambda}{2m}[\\sum_{j=1}^{25}\\sum_{k=1}^{400}(\\theta_{jk}^{(1)})^2 + \\sum_{j=1}^{10}\\sum_{k=1}^{25}(\\theta_{jk}^{(2)})^2] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ypred(Theta1, Theta2, x):\n",
    "    bias = np.array([[1]])\n",
    "    a2 = fwdpropgate(Theta1, x)\n",
    "    a2 = np.concatenate((bias, a2))\n",
    "    a3 = fwdpropgate(Theta2, a2)\n",
    "    return (a2,a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computecost(y, yhat, num_classes):\n",
    "    # yhat represents output unit. Its dimension is (K x 1)\n",
    "    # y represents actual output for a given input sample\n",
    "    # Compute cost for each class\n",
    "    yv = np.zeros((num_classes, 1))\n",
    "    yv[y-1, 0] = 1\n",
    "    cost = -(yv.T.dot(np.log(yhat)) + (1-yv).T.dot(np.log(1 - yhat)))\n",
    "    return cost[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeregterm(Theta1, Theta2, l, num_samples):\n",
    "    # remove the bias column for both theta vectors and flatten it\n",
    "    Theta1 = Theta1[:,1:].flatten()\n",
    "    Theta2 = Theta2[:,1:].flatten()\n",
    "    regparam = Theta1.T.dot(Theta1) + Theta2.T.dot(Theta2)\n",
    "    return ((l*regparam)/(2*num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(ThetaVec, Theta1Size, Theta2Size, X, y, l, pa):\n",
    "    #print(\"In cost\")\n",
    "    # Initialize cost and count variables\n",
    "    count = 0\n",
    "    cost = 0\n",
    "    # Compute length to split Theta parameters from unrolled vector\n",
    "    Theta1Len = Theta1Size[0]*Theta1Size[1]\n",
    "    # Get Theta1 and Theta2\n",
    "    Theta1 = ThetaVec[:Theta1Len].reshape(Theta1Size)\n",
    "    Theta2 = ThetaVec[Theta1Len:].reshape(Theta2Size)\n",
    "    # Get number of samples, features and classes\n",
    "    num_samples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    num_classes = len(set(y.reshape(y.shape[0])))\n",
    "    # For each sample\n",
    "    #   1. Forward propogate\n",
    "    #   2. compute the cost\n",
    "    #   3. Update count for correctly predicted sample\n",
    "    for i in range(num_samples):\n",
    "        # 1\n",
    "        a2, yhat = ypred(Theta1, Theta2, X[i].reshape(num_features, 1))\n",
    "        # 2\n",
    "        cost += (computecost(y[i, 0], yhat, num_classes))\n",
    "        # 3\n",
    "        if(pa == 1):\n",
    "            max_ind = np.argmax(yhat)\n",
    "            if(y[i,0] == (max_ind+1)): count += 1\n",
    "    # Divide cost by num_samples\n",
    "    cost /= num_samples\n",
    "    # Account regularization into cost\n",
    "    if(l > 0):\n",
    "        cost += computeregterm(Theta1, Theta2, l, num_samples)\n",
    "    # Compute accuracy\n",
    "    if(pa == 1):\n",
    "        accuracy = (count/num_samples)\n",
    "        print(\"Accuracy is {:%}, Cost is {}\".format(accuracy, cost))\n",
    "    #print(\"Out cost\")\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost without regularization is 0.28762916516131876\n",
      "Cost with regularization 0.38376985909092354\n"
     ]
    }
   ],
   "source": [
    "# Unroll Theta1 and Theta2 into single vector\n",
    "ThetaVec = np.concatenate((Theta1.ravel(), Theta2.ravel()))\n",
    "# Cost without regularization\n",
    "print(\"Cost without regularization is\", cost(ThetaVec, Theta1.shape, Theta2.shape, X1, y, 0, 0))\n",
    "# Cost with regularization\n",
    "l = 1\n",
    "print(\"Cost with regularization\", cost(ThetaVec, Theta1.shape, Theta2.shape, X1, y, l, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation allows to calculate gradient of the cost function which in turn will be used along with the above cost function by the optimization function to calculate the optimum Theta vectors.\n",
    "1. Implement backpropagation to compute partial derivatives.\n",
    "    * Implement sigmoid gradient function $g'(z) = \\frac{d}{dz}g(z) = g(z)(1 - g(z))$\n",
    "    * Randomly initialize the theta vectors for symmetry breaking.\n",
    "2. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.\n",
    "3. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidgrad(a):\n",
    "    return (a*(1-a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One efective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network. A good choice can be set as\n",
    "$$\\epsilon_{init} = \\frac{\\sqrt(6)}{\\sqrt(s_l + s_{l+1})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401) (10, 26) (10285,)\n"
     ]
    }
   ],
   "source": [
    "#Randomly initialize the weights to small values\n",
    "epsilon_init = 0.12;\n",
    "num_features = X1.shape[1]\n",
    "num_classes = len(set(y.reshape(y.shape[0])))\n",
    "num_units_l1 = num_features\n",
    "num_units_l2 = 25\n",
    "num_units_l3 = num_classes\n",
    "Theta1_r = np.random.random((num_units_l2, num_units_l1)) * 2 * epsilon_init - epsilon_init;\n",
    "Theta2_r = np.random.random((num_units_l3, num_units_l2+1)) * 2 * epsilon_init - epsilon_init;\n",
    "Theta_r = np.concatenate((Theta1_r.ravel(), Theta2_r.ravel()))\n",
    "print(Theta1_r.shape, Theta2_r.shape, Theta_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute gradient, first perform forward propagation and then compute backwards \"error term\" $\\delta_j^{(l)}$ for all units except bias unit from output unit till input unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"BackpropagationUpdates.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient calculation function\n",
    "$$ \\frac{\\partial}{\\partial\\Theta_{ij}^{(l)}}J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m}\\Delta_{ij}^{(l)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ for j = 0$$\n",
    "$$ \\frac{\\partial}{\\partial\\Theta_{ij}^{(l)}}J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m}\\Delta_{ij}^{(l)} + \\frac{\\lambda}{m}\\Theta_{ij}^{(l)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ for j \\ge 1 $$\n",
    "$$ \\Delta^{(l)} = \\Delta^{(l)} + \\delta^{l+1}(a^{(l)})^T $$\n",
    "<br> $m$ - number of samples, $\\lambda$ - Regularization factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(ThetaVec, Theta1Size, Theta2Size, X, y, l, pa):\n",
    "    #print(\"In gradient\")\n",
    "    # Compute length to split Theta parameters from unrolled vector\n",
    "    Theta1Len = Theta1Size[0]*Theta1Size[1]\n",
    "    # Get Theta1 and Theta2\n",
    "    Theta1 = ThetaVec[:Theta1Len].reshape(Theta1Size)\n",
    "    Theta2 = ThetaVec[Theta1Len:].reshape(Theta2Size)\n",
    "    # Initialize gradient vectors\n",
    "    grad_d1 = np.zeros((Theta1.shape))\n",
    "    grad_d2 = np.zeros((Theta2.shape))\n",
    "    # Get number of samples, features and classes\n",
    "    num_samples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    num_classes = len(set(y.reshape(y.shape[0])))\n",
    "    # For each sample:\n",
    "    #   1. Forward propogate\n",
    "    #   2. Backward proppgate\n",
    "    #      a. Compute d3 vector for outer layer\n",
    "    #      b. Compute d2 vector for hidden layer, and remove del value for bias unit\n",
    "    #      c. Accumulate del values d2 and d3 inside gradient matrix\n",
    "    for i in range(num_samples):\n",
    "        # 1\n",
    "        a1 = X[i].reshape(num_features, 1)\n",
    "        a2, yhat = ypred(Theta1, Theta2, a1)\n",
    "        # 2 a\n",
    "        yv = np.zeros((num_classes, 1))\n",
    "        yv[y[i, 0]-1, 0] = 1\n",
    "        d3 = yhat - yv\n",
    "        # 2 b\n",
    "        d2 = Theta2.T.dot(d3) * sigmoidgrad(a2)\n",
    "        d2 = d2[1:,:]\n",
    "        # 2 c\n",
    "        grad_d1 += d2.dot(a1.T)\n",
    "        grad_d2 += d3.dot(a2.T)\n",
    "    # Divide by num_samples\n",
    "    grad_d1 /= num_samples\n",
    "    grad_d2 /= num_samples\n",
    "    # Account regularization into gradient\n",
    "    if(l > 0):\n",
    "        grad_d1[:,1:] += (Theta1[:,1:]*l/num_samples)\n",
    "        grad_d2[:,1:] += (Theta2[:,1:]*l/num_samples)\n",
    "    # Unroll gradient matrix into a single vector\n",
    "    grad = np.concatenate((grad_d1.ravel(), grad_d2.ravel()))\n",
    "    #print(\"Out gradient\")\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Checking\n",
    "Sanity method to check if gradient calculation is working correctly\n",
    "<br>Implement function to calculate approximate gradient matrix\n",
    "<br>If backpropagation implementation is correct, the relative diference between gradient and gradientApprox is less than 1e-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientcheck(ThetaVec, Theta1Size, Theta2Size, X, y, l):\n",
    "    # Compute gradient for original theta parameters\n",
    "    grad = gradient(ThetaVec, Theta1Size, Theta2Size, X, y, l, 0)\n",
    "    # Intitialize gradient vector and epsilon as 10^-4\n",
    "    gradApprox = np.zeros((ThetaVec.shape))\n",
    "    epsilon = 0.0001\n",
    "    # Compute cost for gradApprox, by changing one value at a time\n",
    "    for i in range(ThetaVec.shape[0]):\n",
    "        ThetaVecP = np.copy(ThetaVec)\n",
    "        ThetaVecM = np.copy(ThetaVec)\n",
    "        ThetaVecP[i] += epsilon\n",
    "        ThetaVecM[i] -= epsilon\n",
    "        JP = cost(ThetaVecP, Theta1Size, Theta2Size, X, y, l, 0)\n",
    "        JM = cost(ThetaVecM, Theta1Size, Theta2Size, X, y, l, 0)\n",
    "        gradApprox[i] = (JP - JM)/(2*epsilon)\n",
    "    return (grad, gradApprox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random data of small size for gradient checking otherwise it will be very slow and computationally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandData(rows, cols):\n",
    "    num_elems = rows * (cols+1)\n",
    "    W = (np.sin(np.array(np.arange(1,num_elems+1))))/10\n",
    "    return W.reshape(rows, cols+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.39475668e-12 -7.73273806e-13  4.74464218e-13 -1.46139698e-12\n",
      "  4.39316118e-12  3.20009384e-12 -1.72885664e-12  1.85249385e-12\n",
      " -4.33469927e-12 -2.62379354e-12  2.23259605e-12 -3.32482479e-12\n",
      " -9.39690860e-12 -1.67635003e-12  2.02796287e-12 -4.53751620e-13\n",
      " -5.75027005e-12 -8.03537792e-13  3.46705520e-12  1.69384298e-12\n",
      "  5.71509506e-12  5.33406652e-13  9.01723141e-13 -2.19763097e-12\n",
      "  1.76128556e-12  2.07583950e-13  6.26139418e-12  4.17268997e-12\n",
      "  1.53636825e-12  4.15208146e-12 -5.29104538e-13  1.52722279e-12\n",
      "  6.28821994e-12  2.27812214e-12  1.79010973e-12 -1.45929102e-12\n",
      "  5.19209675e-13 -3.99613675e-12]\n"
     ]
    }
   ],
   "source": [
    "input_layer_size = 3;\n",
    "hidden_layer_size = 5;\n",
    "num_labels = 3;\n",
    "m = 5;\n",
    "ThetaT1 = getRandData(hidden_layer_size, input_layer_size)\n",
    "ThetaT2 = getRandData(num_labels, hidden_layer_size);\n",
    "XT  = getRandData(m, input_layer_size - 1);\n",
    "XT = np.hstack((np.ones((XT.shape[0], 1)), XT))\n",
    "yt  = (1 + np.mod(np.arange(1,m+1), num_labels)).reshape(m, 1)\n",
    "ThetaVecT = np.concatenate((ThetaT1.ravel(), ThetaT2.ravel()))\n",
    "(grad, gradApprox) = gradientcheck(ThetaVecT, ThetaT1.shape, ThetaT2.shape, XT, yt, 1)\n",
    "print(grad - gradApprox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning parameters using Advanced Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optimization method in scipy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 95.780000%, Cost is 0.44785840135502786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44785840135502786"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 1\n",
    "Result = op.minimize(fun = cost, \n",
    "                         x0 = Theta_r, \n",
    "                         args = (Theta1_r.shape, Theta2_r.shape, X1, y, l, 0),\n",
    "                         method = 'CG',\n",
    "                         jac = gradient,\n",
    "                        options = {'maxiter': 50});\n",
    "optimal_theta = Result.x;\n",
    "cost(optimal_theta, Theta1_r.shape, Theta2_r.shape, X1, y, l, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

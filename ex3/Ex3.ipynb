{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Multi-class Classification and Neural Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement one-vs-all logistic regression and neural networks to recognize hand-written digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement one-vs-all logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement one-vs-all logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import scipy.optimize as op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from a matlab file. The data is in the dictionary format with X representing 5000 20x20 pixel images in a 5000 x 400 matrix and y representing the number corresponding to the image in 5000 x 1 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'X', 'y'])\n",
      "(5000, 400) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "pixelsmat = scipy.io.loadmat('ex3data1.mat')\n",
    "print(pixelsmat.keys())\n",
    "X = pixelsmat['X']\n",
    "y = pixelsmat['y']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the different classes in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different classes {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
      "Total number of classes 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Different classes\", set(y.reshape(y.shape[0])))\n",
    "num_classes = len(set(y.reshape(y.shape[0])))\n",
    "print(\"Total number of classes\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convinience, lets replace class '10' by class '0', as indexing in python starts from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different classes {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "y = np.where(y == 10, 0, y)\n",
    "print(\"Different classes\", set(y.reshape(y.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the number of pixels variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 20\n"
     ]
    }
   ],
   "source": [
    "numpixels1d = X.shape[1]\n",
    "numpixels2d = int(np.sqrt(numpixels1d))\n",
    "print(numpixels1d, numpixels2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize images using randomly selected 100 rows from X matrix. The image can be displayed using imshow function available in matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img = 100\n",
    "rows = cols = int(np.sqrt(100))\n",
    "indices = np.random.randint(0,5000, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgArr = np.zeros((num_img, numpixels1d))\n",
    "for i in range(np.size(indices)):\n",
    "    imgArr[i] = X[indices[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcb2c3fe02e4be5b642712a4ed841e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "for i in range(1, np.size(indices)+1):\n",
    "    img = imgArr[i-1].reshape(numpixels2d, numpixels2d)\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.T, cmap='gray', origin=\"upper\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a vectorized regularized logistic regression  avoiding any \"for loops\" to make it efficient.\n",
    "Cost function for logistic regression\n",
    "$$ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m[-y^{(i)}log(h_\\theta(x^{(i)})) - (1 - y^{(i)})log(1 - h_\\theta(x^{(i)})] + \\frac{\\lambda}{2m}\\sum_{j=1}^n\\theta_j^2 $$\n",
    "The logistic regression hypothesis is defined as\n",
    "$$ h_\\theta(x) = g(\\theta^Tx) $$\n",
    "where g is the Sigmoid function\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the notion that $\\theta^TX = X^T\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize X1 to have a column 1 vector (bias vector) to be appended at beginning and y_set to contain unique classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 shape (5000, 401)\n"
     ]
    }
   ],
   "source": [
    "y_set = set(y.reshape(y.shape[0]))\n",
    "X1 = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "print(\"X1 shape\", X1.shape)\n",
    "num_samples = X1.shape[0]\n",
    "num_features = X1.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1/(1+(np.exp(-z))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computecostreg(theta, X, y, l):\n",
    "    # Reshape the theta vector into 2D array \n",
    "    theta = theta.reshape((num_features,1))\n",
    "    \n",
    "    # Compute hypothesis\n",
    "    yhat = sigmoid(X.dot(theta))\n",
    "    \n",
    "    # Compute the cost value\n",
    "    #cost_a = y * np.log(yhat)\n",
    "    #cost_b = (1 - y) * np.log(1 - yhat)\n",
    "    #cost = (np.sum(cost_a) + np.sum(cost_b))/(-X.shape[0])\n",
    "    cost = (y.T.dot(np.log(yhat)) + (1-y).T.dot(np.log(1 - yhat)))/(-num_samples)\n",
    "    \n",
    "    theta_sub = theta[1:,:]\n",
    "    #regparam = np.sum(theta_sub * theta_sub)\n",
    "    regparam = theta_sub.T.dot(theta_sub)\n",
    "    cost += (regparam * l)/(2 * num_samples)\n",
    "    return cost  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optimization method in scipy library. This needs the Cost Funcion, Gradient function , $\\theta$, X and y values as parameters.\n",
    "The gradient function for regularized logistic regression\n",
    "for j  = 0 $$Gradient = \\frac{1}{m}\\sum_{i=1}^m[(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}]$$\n",
    "for j >= 1 $$Gradient = \\frac{1}{m}\\sum_{i=1}^m[(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}] + \\frac{\\lambda}{m}\\theta_j $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientreg(theta, X, y, l):\n",
    "    # Reshape the theta vector into 2D array\n",
    "    theta = theta.reshape((num_features,1))\n",
    "    \n",
    "    # Compute hypothesis\n",
    "    yhat = sigmoid(X.dot(theta))\n",
    "    \n",
    "    # Compute error vector\n",
    "    diff = yhat - y\n",
    "    \n",
    "    # Compute gradient and return it in 1D format\n",
    "    grad = (X.T.dot(diff))/(num_samples)\n",
    "    theta_sub = theta[1:,:]\n",
    "    grad[1:,:] += (theta_sub * l)/(num_samples)\n",
    "    return grad.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optimization method in scipy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding theta values for 0\n",
      "Finding theta values for 1\n",
      "Finding theta values for 2\n",
      "Finding theta values for 3\n",
      "Finding theta values for 4\n",
      "Finding theta values for 5\n",
      "Finding theta values for 6\n",
      "Finding theta values for 7\n",
      "Finding theta values for 8\n",
      "Finding theta values for 9\n",
      "final cost [[0.02014754]]\n",
      "final cost [[0.02696008]]\n",
      "final cost [[0.06844193]]\n",
      "final cost [[0.07170591]]\n",
      "final cost [[0.05205027]]\n",
      "final cost [[0.07685827]]\n",
      "final cost [[0.03478613]]\n",
      "final cost [[0.04672434]]\n",
      "final cost [[0.09274595]]\n",
      "final cost [[0.08931062]]\n"
     ]
    }
   ],
   "source": [
    "l = 1\n",
    "init_theta = np.zeros(X1.shape[1])\n",
    "optimal_theta = np.zeros((num_classes, num_features))\n",
    "for i in y_set:\n",
    "    y_i = (y == i)\n",
    "    print(\"Finding theta values for\", i)\n",
    "    Result = op.minimize(fun = computecostreg, \n",
    "                                     x0 = init_theta, \n",
    "                                     args = (X1, y_i, l),\n",
    "                                     method = 'TNC',\n",
    "                                     jac = gradientreg);\n",
    "    optimal_theta[i] = Result.x;\n",
    "    #Result = op.fmin_tnc(func=computecostreg, x0=init_theta, fprime=gradientreg, args=(X1, y_i, l))\n",
    "    #optimal_theta[ind] = Result[0];\n",
    "for i in y_set:\n",
    "    y_i = (y == i)\n",
    "    print(\"final cost\", computecostreg(optimal_theta[i].reshape((num_features, 1)), X1, y_i, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redoing the same exercise using in-built Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal_theta shape (10, 401)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anupam\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(fit_intercept=True, C = 1e15, solver='saga')\n",
    "clf.fit(X, y.ravel())\n",
    "\n",
    "optimal_theta2 = np.hstack((clf.intercept_.reshape(num_classes,1), clf.coef_))\n",
    "print(\"optimal_theta shape\", optimal_theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the accuracy of the weights on the training set. Use One-vs-all Prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNumber(theta, X):\n",
    "    yhat = sigmoid(X.T.dot(theta))\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcaccuracy(optimal_theta_arr):\n",
    "    count = 0\n",
    "    for i in range(num_samples):\n",
    "        max_prob = 0\n",
    "        for j in range(optimal_theta_arr.shape[0]):\n",
    "            prob_j = predictNumber(optimal_theta_arr[j].reshape((num_features, 1)), X1[i].reshape((num_features, 1)))\n",
    "            if(j == 0 or (prob_j > max_prob)):\n",
    "                max_prob = prob_j\n",
    "                out_class = j\n",
    "        if(y[i,0] == out_class): count += 1\n",
    "    print(\"Accuracy is {:%}\".format(count/num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 94.460000%\n"
     ]
    }
   ],
   "source": [
    "calcaccuracy(optimal_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network will be able to represent complex models that form non-linear hypotheses. To do this using multi-class logistic regression, more features (Polynomial Features) would have to be added and this makes the process more expensive.</br>\n",
    "This assignment will use already predicted parameters from a neural network that we have already trained.</br>\n",
    "Goal is to implement the feedforward propagation algorithm to use our weights for prediction.</br>\n",
    "In next week's exercise, you will write the backpropagation algorithm for learning the neural network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NeuralNetworkModel.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from a matlab file. The data is in the dictionary format with Theta1 representing weights to map input layer to hidden layer and Theta2 representing weights to map hidden layer to output layer.</br>\n",
    "Input layer as 401 units representing 400 features for 20x20 pixels and one bias unit. Hidden Layer has 25 units plus one bias unit. Output layer has 10 units for 10 classes in our original dataset.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'Theta1', 'Theta2'])\n",
      "(25, 401) (10, 26)\n"
     ]
    }
   ],
   "source": [
    "weightsmat = scipy.io.loadmat('ex3weights.mat')\n",
    "print(weightsmat.keys())\n",
    "Theta1 = weightsmat['Theta1']\n",
    "Theta2 = weightsmat['Theta2']\n",
    "print(Theta1.shape, Theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for forward propagation and then compute prediction for all samples and the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwdpropgate(theta, x):\n",
    "    z = theta.dot(x)\n",
    "    a = sigmoid(z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 97.520000%\n"
     ]
    }
   ],
   "source": [
    "bias = np.array([[1]])\n",
    "out_arr = np.zeros(((num_samples, 1)))\n",
    "count = 0\n",
    "for i in range(num_samples):\n",
    "    a2 = fwdpropgate(Theta1, X1[i].reshape(num_features, 1))\n",
    "    a2 = np.concatenate((bias, a2))\n",
    "    a3 = fwdpropgate(Theta2, a2)\n",
    "    max_ind = np.argmax(a3)\n",
    "    max_ind = (max_ind+1)%10\n",
    "    if(y[i,0] == (max_ind)): count += 1\n",
    "print(\"Accuracy is {:%}\".format(count/num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
